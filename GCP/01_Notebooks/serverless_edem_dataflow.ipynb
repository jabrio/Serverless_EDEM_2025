{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Apache Beam Basics**\n",
    "\n",
    "Description: This notebook will teach you the basics of the Apache Beam Programming Model:\n",
    "\n",
    "- Apache Beam Basics: Pipeline, PCollection, Transforms & PTransforms.\n",
    "   - GroupByKey\n",
    "   - CoGroupByKey\n",
    "   - Flatten\n",
    "   - Partition\n",
    "- Introducing complexity: DoFn.\n",
    "   - DoFn vs Map\n",
    "   - DoFn LifeCycle\n",
    "   - DoFn Stateful Processing\n",
    "- Advanced: Streaming.\n",
    "   - Fixed Windows\n",
    "   - Sliding Windows\n",
    "   - PubSub to Bigquery\n",
    "\n",
    "\n",
    "EDEM. Master Big Data & Cloud 2024/2025<br>\n",
    "Professor: Javi Briones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import apache_beam as beam\n",
    "\n",
    "from apache_beam.runners.interactive.interactive_runner import InteractiveRunner\n",
    "import apache_beam.runners.interactive.interactive_beam as ib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Logs\n",
    "\n",
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format=\"serverless-edem-%(message)s\")\n",
    "\n",
    "# Suppress Apache Beam logs\n",
    "logging.getLogger(\"apache_beam\").setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apache Beam Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apache Beam is a unified programming model for parallel data processing that provides a rich set of transformations to efficiently manipulate and process both batch and streaming data across multiple execution environments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Apache Beam Core Concepts** \n",
    "\n",
    "##### **Pipeline**\n",
    "A **Pipeline** represents the entire workflow for processing data in Apache Beam. It defines the sequence of transformations applied to input data to produce outputs.\n",
    "\n",
    "##### **PCollection**\n",
    "A **PCollection** is an immutable, distributed dataset that serves as the input and output for transformations in a pipeline. It can handle bounded (batch) or unbounded (streaming) data.\n",
    "\n",
    "##### **Transformations**\n",
    "A **Transformation** is an operation applied to a PCollection to produce one or more output PCollections. Common transformations include Map, FlatMap, GroupByKey, and Combine.\n",
    "\n",
    "##### **PTransform**\n",
    "A **PTransform** is a reusable and composable abstraction that encapsulates one or more transformations. It simplifies pipelines by modularizing complex workflows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Exercise 01: What is a PCollection?\n",
    "\n",
    "PCollection: A distributed dataset in Apache Beam\n",
    "\"\"\"\n",
    "\n",
    "#  Create and explore a simple PCollection\n",
    "\n",
    "with beam.Pipeline(InteractiveRunner()) as pipeline:\n",
    "    \n",
    "    # Create a PCollection\n",
    "    numbers = pipeline | \"Create Numbers\" >> beam.Create([1, 2, 3, 4, 5])\n",
    "\n",
    "    # Log output\n",
    "    numbers | \"Log Elements\" >> beam.Map(logging.info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Exercise 02: Declaring a Pipeline\n",
    "\n",
    "Pipeline: Defines the workflow of data transformations.\n",
    "\"\"\"\n",
    "\n",
    "# Interactive pipeline\n",
    "with beam.Pipeline(InteractiveRunner()) as pipeline:\n",
    "    \n",
    "    # Create a PCollection and perform a transformation\n",
    "    squared_numbers = (\n",
    "        pipeline\n",
    "        | \"Create Numbers\" >> beam.Create([1, 2, 3, 4, 5])\n",
    "        | \"Square Numbers\" >> beam.Map(lambda x: x * x)\n",
    "    )\n",
    "\n",
    "    # Log output\n",
    "    squared_numbers | \"Log Squared Numbers\" >> beam.Map(logging.info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Exercise 03: Transformations\n",
    "\n",
    "Transforms one or more input PCollections into one or more output PCollections.\n",
    "\"\"\"\n",
    "\n",
    "# Interactive pipeline\n",
    "with beam.Pipeline(InteractiveRunner()) as pipeline:\n",
    "\n",
    "    (\n",
    "        pipeline\n",
    "            # Input\n",
    "            | \"Input PCollection: Read Text From File\" >> beam.io.ReadFromText('./input_text.txt')\n",
    "            # Transformations\n",
    "            | \"FlatMap\" >> beam.FlatMap(lambda z: z.split())\n",
    "            | \"Map\" >> beam.Map(lambda x: (x,1))\n",
    "            | \"Combine\" >> beam.CombinePerKey(sum)\n",
    "            # Output\n",
    "            | \"Output PCollection: Print\" >> beam.Map(logging.info)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Exercise 04: Reuse Transformations (PTransforms)\n",
    "\n",
    "PTransform: A collection of transformations packaged as a reusable unit for complex workflows.\n",
    "\"\"\"\n",
    "\n",
    "class MyCustomTransform(beam.PTransform):\n",
    "    \n",
    "    def expand(self, pcoll):\n",
    "        return (\n",
    "            pcoll\n",
    "            | \"FlatMap\" >> beam.FlatMap(lambda z: z.split())\n",
    "            | \"Map\" >> beam.Map(lambda x: (x,1))\n",
    "            | \"Combine\" >> beam.CombinePerKey(sum)\n",
    "        )\n",
    "\n",
    "with beam.Pipeline(InteractiveRunner()) as pipeline:\n",
    "\n",
    "    (\n",
    "        pipeline\n",
    "            | \"Input PCollection: Read Text From File\" >> beam.io.ReadFromText('./input_text.txt')\n",
    "            | \"Apply Custom Transform\" >> MyCustomTransform()\n",
    "            | \"Output PCollection: Print\" >> beam.Map(logging.info)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Apache Beam Transformations** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**- GroupByKey**\n",
    "\n",
    "It groups the elements of a data bundle by a common key, producing a collection where each key is unique and associated with a list of values that share that key.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" GroupByKey \"\"\"\n",
    "\n",
    "with beam.Pipeline(InteractiveRunner()) as p:\n",
    "\n",
    "    data = (p | \"PCollection\" >> beam.Create([('Spain', 'Valencia'), ('Spain','Barcelona'), ('France', 'Paris')]))\n",
    "\n",
    "    (data \n",
    "        | \"Combined\" >> beam.GroupByKey()\n",
    "        | \"Print\" >> beam.Map(logging.info))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**- CoGroupByKey** \n",
    "\n",
    "It merges two PCollections by key, producing pairs where each key is associated with lists of elements from both input collections. This is useful for operations involving data from two different sources.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with beam.Pipeline(InteractiveRunner()) as p:\n",
    "\n",
    "    p1 = p | \"PCollection 01\" >> beam.Create([('Spain', 'Valencia'), ('Spain','Barcelona'), ('France', 'Paris')])\n",
    "    p2 = p | \"PCollection 02\" >> beam.Create([('Spain', 'Madrid'), ('Spain','Alicante'), ('France', 'Lyon')])\n",
    "\n",
    "    data = ((p1,p2) | beam.CoGroupByKey())\n",
    "\n",
    "    data | \"Print\" >> beam.Map(logging.info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**- Flatten**  \n",
    "\n",
    "It merges multiple PCollections into a single PCollection, combining their elements into one flat structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with beam.Pipeline(InteractiveRunner()) as p:\n",
    "\n",
    "    p1 = p | \"PCollection 01\" >> beam.Create(['New York', 'Los Angeles', 'Miami', 'Chicago'])\n",
    "    p2 = p | \"Pcollection 02\" >> beam.Create(['Madrid', 'Barcelona', 'Valencia', 'Malaga'])\n",
    "    p3 = p | \"Pcollection 03\" >> beam.Create(['London','Manchester', 'Liverpool'])\n",
    "\n",
    "    merged = ((p1,p2,p3)| beam.Flatten())\n",
    "\n",
    "    merged | beam.Map(logging.info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**- Partition**  \n",
    "\n",
    "It splits a PCollection into multiple partitions based on defined criteria, enabling parallel and distributed processing of subsets of the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries = ['Spain', 'USA', 'Switzerland']\n",
    "\n",
    "def partition_fn(country,num_countries):\n",
    "    return countries.index(country['country'])\n",
    "\n",
    "with beam.Pipeline(InteractiveRunner()) as p:\n",
    "\n",
    "        p1,p2,p3 = (\n",
    "                p \n",
    "                | \"PCollection\" >> beam.Create([\n",
    "                        {'country': 'Spain', 'city': 'Valencia'},\n",
    "                        {'country': 'Spain', 'city': 'Barcelona'},\n",
    "                        {'country': 'USA', 'city': 'New York'},\n",
    "                        {'country': 'Switzerland', 'city': 'Zurich'},\n",
    "                        {'country': 'Switzerland', 'city': 'Geneva'}  \n",
    "                ])\n",
    "                | \"partition\" >> beam.Partition(partition_fn, len(countries))\n",
    "        )\n",
    "\n",
    "        p2 |  beam.Map(logging.info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducing complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A. When to Use DoFn Instead of Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Exercise 05: This exercise shows how to achieve the same transformation\n",
    "    using both Map and ParDo with DoFn.\n",
    "\n",
    "Use Map for simple transformations where the logic is inline and does not require lifecycle methods.\n",
    "Use ParDo with DoFn for more complex operations requiring setup, cleanup, or state management.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Map \"\"\"\n",
    "with beam.Pipeline(InteractiveRunner()) as pipeline:\n",
    "\n",
    "    (\n",
    "        pipeline\n",
    "            | \"Input PCollection: Read Text From File\" >> beam.io.ReadFromText('./input_text.txt')\n",
    "            | \"FlatMap\" >> beam.FlatMap(lambda z: z.split())\n",
    "            | \"Map\" >> beam.Map(lambda x: x.upper())\n",
    "            | \"Output PCollection: Print\" >> beam.Map(logging.info)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"ParDo with DoFn\"\"\"\n",
    "\n",
    "class UpperCaseDoFn(beam.DoFn):\n",
    "\n",
    "    def setup(self):\n",
    "        logging.info(\"Loading model...\")\n",
    "        self.model = lambda x: x.upper()\n",
    "\n",
    "    def process(self, element):\n",
    "        yield self.model(element)\n",
    "\n",
    "    def teardown(self):\n",
    "        logging.info(\"Releasing model resources.\")\n",
    "\n",
    "with beam.Pipeline(InteractiveRunner()) as pipeline:\n",
    "\n",
    "    (\n",
    "        pipeline\n",
    "            | \"Input PCollection: Read Text From File\" >> beam.io.ReadFromText('./input_text.txt')\n",
    "            | \"FlatMap\" >> beam.FlatMap(lambda z: z.split())\n",
    "            | \"DoFn\" >> beam.ParDo(UpperCaseDoFn())\n",
    "            | \"Output PCollection: Print\" >> beam.Map(logging.info)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Exploring the DoFn LifeCycle**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Lifecycle of a **DoFn** in Apache Beam refers to the stages that an instance of the `DoFn` class goes through from initialization to completion within the context of a *ParDo* transformation. Below are the key phases of the DoFn life cycle:\n",
    "\n",
    "1. **Setup (Initialization):**  \n",
    "   - **Goal:** This phase occurs once for each instance of `DoFn` before processing begins.  \n",
    "   - **Process:** Initialization tasks, such as allocating resources, establishing connections, or loading models, are performed in the `setup(self)` method.\n",
    "\n",
    "2. **Processing Elements (Process):**  \n",
    "   - **Goal:** This phase executes the main logic for each input element in the data bundle.  \n",
    "   - **Process:** The core processing logic is implemented in the `process(self, element)` method. This method is called for each element in the input `PCollection` and defines how the data is transformed.\n",
    "\n",
    "3. **Start Bundle and Finish Bundle:**  \n",
    "   - **Goal:** These phases occur once before (`start_bundle(self)`) and after (`finish_bundle(self)`) processing all elements in a bundle. A bundle represents a chunk of data processed in parallel.  \n",
    "   - **Process:** Bundle-specific setup and cleanup tasks, such as initializing or finalizing temporary states or resources used within the bundle, are performed here.\n",
    "\n",
    "4. **Teardown (Finalization):**  \n",
    "   - **Goal:** This phase occurs once after all elements have been processed.  \n",
    "   - **Process:** The `teardown(self)` method handles final resource cleanup, such as closing database connections, releasing memory, or terminating external processes.\n",
    "\n",
    "---\n",
    "\n",
    "The lifecycle of a **DoFn** provides a structured framework for managing resources and processing logic in *ParDo* transformations. Each instance is created, initialized, processes elements, and performs cleanup in a controlled and efficient manner. This ensures proper resource management and facilitates robust, scalable data processing pipelines.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use ParDo with DoFn for tasks requiring state, lifecycle, or multiple outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Exercise: Understanding the lifecycle methods (setup, start_bundle, process, finish_bundle, and teardown)\n",
    " and how they are invoked during pipeline execution\n",
    "\"\"\"\n",
    "\n",
    "class LifecycleDoFn(beam.DoFn):\n",
    "\n",
    "    def setup(self):\n",
    "        logging.info(\"Setting up resources.\")\n",
    "\n",
    "    def start_bundle(self):\n",
    "        logging.info(\"Starting a bundle.\")\n",
    "\n",
    "    def process(self, element):\n",
    "        logging.info(f\"Processing element: {element}\")\n",
    "        yield element.upper()\n",
    "\n",
    "    def finish_bundle(self):\n",
    "        logging.info(\"Finishing a bundle.\")\n",
    "\n",
    "    def teardown(self):\n",
    "        logging.info(\"Tearing down resources.\")\n",
    "\n",
    "with beam.Pipeline(InteractiveRunner()) as pipeline:\n",
    "\n",
    "    (\n",
    "        pipeline\n",
    "            | \"Input PCollection: Read Text From File\" >> beam.io.ReadFromText('./input_text.txt')\n",
    "            | \"FlatMap\" >> beam.FlatMap(lambda z: z.split())\n",
    "            | \"Apply Lifecycle DoFn\" >> beam.ParDo(LifecycleDoFn())\n",
    "            | \"Output PCollection: Print\" >> beam.Map(logging.info)\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **DoFn: Stateful Processing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Allows operations that depend on previous or cumulative data.\n",
    "- Reduces the need to rely on external systems, such as databases or caching systems.\n",
    "- Efficient, as the state is stored locally for each key and window.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "from apache_beam.transforms.userstate import CombiningValueStateSpec\n",
    "\n",
    "\"\"\"\n",
    "Exercise: The state is persisted across multiple calls to process()\n",
    "\"\"\"\n",
    "\n",
    "class StatefulDoFn(beam.DoFn):\n",
    "    \n",
    "    # Define a state spec to maintain counts\n",
    "    state_spec = CombiningValueStateSpec(\"count\", sum)\n",
    "\n",
    "    def process(self, element, state=beam.DoFn.StateParam(state_spec)):\n",
    "        k,v = element\n",
    "        state.add(1)\n",
    "        yield f\"Key: {k}, Count: {state.read()}\"\n",
    "    \n",
    "# Interactive pipeline\n",
    "with beam.Pipeline(InteractiveRunner()) as pipeline:\n",
    "\n",
    "    (\n",
    "        pipeline\n",
    "            | \"Input PCollection: Read Text From File\" >> beam.io.ReadFromText('./input_text.txt')\n",
    "            | \"FlatMap\" >> beam.FlatMap(lambda z: z.split())\n",
    "            | \"Map\" >> beam.Map(lambda x: (x.lower(),1))\n",
    "            | \"Count Events Per Key\" >> beam.ParDo(StatefulDoFn())\n",
    "            | \"Output PCollection: Print\" >> beam.Map(logging.info)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Streaming** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GCP Setup\n",
    "\n",
    "- PubSub Topics\n",
    "\n",
    "```\n",
    "gcloud pubsub topics create <TOPIC_NAME>\n",
    "```\n",
    "\n",
    "- PubSub Subscriptions\n",
    "\n",
    "```\n",
    "gcloud pubsub subscriptions create <SUBSCRIPTION_NAME> --topic=<TOPIC_NAME>\n",
    "```\n",
    "\n",
    "- Google Cloud Storage Bucket  \n",
    "\n",
    "```\n",
    "gcloud storage mb gs://<BUCKET_NAME> --location=<REGION_ID>\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables\n",
    "project_id = \"\"\n",
    "subscription_name = \"\"\n",
    "bq_dataset = \"\"\n",
    "bq_table = \"\"\n",
    "bucket_name = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Window in Apache Beam**\n",
    "  \n",
    "A window in Apache Beam defines a time frame for **organizing and grouping data elements** during processing, enabling time-based operations.\n",
    "\n",
    "---\n",
    "\n",
    "**Types of Windows:**\n",
    "\n",
    "- **Fixed Window:** \n",
    "\n",
    "  Divides elements into fixed time intervals, segmenting the PCollection into evenly spaced, time-based windows.\n",
    "\n",
    "- **Sliding Window:**  \n",
    "\n",
    "  Creates overlapping windows with a specified size and stride, enabling continuous analysis of data over time.\n",
    "\n",
    "- **Session Window:**  \n",
    "\n",
    "  Groups elements based on **contiguous temporal activity**, where windows are dynamically defined by an **inactivity gap** between events, making it ideal for capturing logical sessions in streaming data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### - **Fixed Windows**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "import json\n",
    "import logging\n",
    "from apache_beam.options.pipeline_options import PipelineOptions\n",
    "\n",
    "def decode_message(msg):\n",
    "\n",
    "    output = msg.decode('utf-8')\n",
    "\n",
    "    logging.info(\"New PubSub Message: %s\", output)\n",
    "    \n",
    "    return json.loads(output)['temp']\n",
    "\n",
    "class OutputDoFn(beam.DoFn):\n",
    "\n",
    "    def process(self, element):\n",
    "        yield element\n",
    "\n",
    "def run():\n",
    "    with beam.Pipeline(options=PipelineOptions(streaming=True, save_main_session=True)) as p:\n",
    "        (\n",
    "            p \n",
    "            | \"ReadFromPubSub\" >> beam.io.ReadFromPubSub(subscription=f'projects/{project_id}/subscriptions/{subscription_name}')\n",
    "            | \"Decode msg\" >> beam.Map(decode_message)\n",
    "            | \"Fixed Window\" >> beam.WindowInto(beam.window.FixedWindows(10))\n",
    "            | \"Combine\" >> beam.CombineGlobally(sum).without_defaults()\n",
    "            | \"Print\" >> beam.Map(print)\n",
    "        )\n",
    "\n",
    "# Run Process\n",
    "logging.info(\"The process started\")\n",
    "run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### - **Sliding Windows**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "import json\n",
    "import logging\n",
    "from apache_beam.options.pipeline_options import PipelineOptions\n",
    "\n",
    "def decode_message(msg):\n",
    "\n",
    "    output = msg.decode('utf-8')\n",
    "\n",
    "    logging.info(\"New PubSub Message: %s\", output)\n",
    "\n",
    "    return output['temp']\n",
    "\n",
    "class OutputDoFn(beam.DoFn):\n",
    "\n",
    "    def process(self, element):\n",
    "        yield element\n",
    "\n",
    "def run():\n",
    "    with beam.Pipeline(options=PipelineOptions(streaming=True, save_main_session=True)) as p:\n",
    "        (\n",
    "            p \n",
    "            | \"ReadFromPubSub\" >> beam.io.ReadFromPubSub(subscription=f'projects/{project_id}/subscriptions/{subscription_name}')\n",
    "            | \"Decode msg\" >> beam.Map(decode_message)\n",
    "            | \"Sliding Window\" >> beam.WindowInto(beam.window.SlidingWindows(size=60, period=20))\n",
    "            | \"Combine\" >> beam.CombinePerKey(sum)\n",
    "            | \"Print\" >> beam.Map(print)\n",
    "        )\n",
    "\n",
    "# Run Process\n",
    "logging.info(\"The process started\")\n",
    "run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PubSub to BigQuery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "import json\n",
    "import logging\n",
    "from apache_beam.options.pipeline_options import PipelineOptions\n",
    "\n",
    "def decode_message(msg):\n",
    "\n",
    "    output = msg.decode('utf-8')\n",
    "\n",
    "    logging.info(\"New PubSub Message: %s\", output)\n",
    "\n",
    "    return json.loads(output)\n",
    "\n",
    "def run():\n",
    "    with beam.Pipeline(options=PipelineOptions(\n",
    "        streaming=True,\n",
    "        save_main_session=True,\n",
    "        job_name = \"pubsub-to-bigquery-edem\",\n",
    "        project=project_id,\n",
    "        runner=\"DataflowRunner\",\n",
    "        temp_location=f\"gs://{bucket_name}/tmp\",\n",
    "        staging_location=f\"gs://{bucket_name}/staging\",\n",
    "        region=\"europe-southwest1\"\n",
    "    )) as p:\n",
    "        \n",
    "        (\n",
    "            p \n",
    "            | \"ReadFromPubSub\" >> beam.io.ReadFromPubSub(subscription=f'projects/{project_id}/subscriptions/{subscription_name}')\n",
    "            | \"decode msg\" >> beam.Map(decode_message)\n",
    "            | \"Write to BigQuery\" >> beam.io.WriteToBigQuery(\n",
    "                table = f\"{project_id}:{bq_dataset}.{bq_table}\", # Required Format: PROJECT_ID:DATASET.TABLE\n",
    "                schema='name:STRING', # Required Format: field_name:TYPE\n",
    "                create_disposition=beam.io.BigQueryDisposition.CREATE_IF_NEEDED,\n",
    "                write_disposition=beam.io.BigQueryDisposition.WRITE_APPEND\n",
    "            )\n",
    "        )\n",
    "\n",
    "# Run Process\n",
    "logging.info(\"The process started\")\n",
    "run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
